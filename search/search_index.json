{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"404: Page Not Found","text":"<p>We\u2019re redirecting you to the Overview page...</p>"},{"location":"api/desyncclient/","title":"DesyncClient","text":"<p>The <code>DesyncClient</code> class provides a high-level interface to the Desync Search API, managing individual searches, bulk operations, domain crawling, and credit balance checks. Below are its primary methods, each listed with a short heading. The full method signature appears under each heading in a code block.</p>"},{"location":"api/desyncclient/#init","title":"init()","text":"<p>Signature: Python<pre><code>def __init__(self, user_api_key=\"\", developer_mode=False)\n</code></pre></p> <p>Description: Initializes the client with the provided API key or reads it from the <code>DESYNC_API_KEY</code> environment variable. If <code>developer_mode</code> is <code>True</code>, the client uses a test endpoint; otherwise, it uses the production endpoint.</p> <p>Parameters:</p> <ul> <li>user_api_key (str, optional): Your Desync API key.  </li> <li>developer_mode (bool, optional): Toggles between test and production endpoints.</li> </ul> <p>Example: Python<pre><code>from desync_search import DesyncClient\n\nclient = DesyncClient(user_api_key=\"YOUR_API_KEY\", developer_mode=False)\n</code></pre></p>"},{"location":"api/desyncclient/#search","title":"search()","text":"<p>Signature: Python<pre><code>def search(\n    self,\n    url,\n    search_type=\"stealth_search\",\n    scrape_full_html=False,\n    remove_link_duplicates=True\n) -&gt; PageData\n</code></pre></p> <p>Description: Performs a single search on a specified URL. Returns a <code>PageData</code> object containing the page\u2019s text, links, timestamps, and other metadata.</p> <p>Parameters:</p> <ul> <li>url (str): The URL to scrape.  </li> <li>search_type (str): <code>\"stealth_search\"</code> (default, 10 credits) or <code>\"test_search\"</code> (1 credit).  </li> <li>scrape_full_html (bool): If <code>True</code>, returns the full HTML content.  </li> <li>remove_link_duplicates (bool): If <code>True</code>, removes duplicate links from the results.</li> </ul> <p>Example: Python<pre><code>result = client.search(\"https://example.com\")\nprint(result.text_content)\n</code></pre></p>"},{"location":"api/desyncclient/#bulk_search","title":"bulk_search()","text":"<p>Signature: Python<pre><code>def bulk_search(\n    self,\n    target_list,\n    extract_html=False\n) -&gt; dict\n</code></pre></p> <p>Description: Initiates an asynchronous bulk search on up to 1000 URLs at once. Returns a dictionary containing a <code>bulk_search_id</code> and other metadata.</p> <p>Parameters:</p> <ul> <li>target_list (list[str]): List of URLs to process.  </li> <li>extract_html (bool): If <code>True</code>, includes the full HTML content in results.</li> </ul> <p>Example: Python<pre><code>bulk_info = client.bulk_search(\n    target_list=[\"https://example.com\", \"https://another-example.net\"]\n)\nprint(bulk_info[\"bulk_search_id\"])\n</code></pre></p>"},{"location":"api/desyncclient/#list_available","title":"list_available()","text":"<p>Signature: Python<pre><code>def list_available(\n    self,\n    url_list=None,\n    bulk_search_id=None\n) -&gt; list\n</code></pre></p> <p>Description: Retrieves minimal data about previously collected search results (IDs, domains, timestamps, etc.). Returns a list of <code>PageData</code> objects with limited fields.</p> <p>Parameters:</p> <ul> <li>url_list (list[str], optional): Filters results by specific URLs.  </li> <li>bulk_search_id (str, optional): Filters results by a particular bulk search ID.</li> </ul> <p>Example: Python<pre><code>partial_records = client.list_available(bulk_search_id=\"some-bulk-id\")\nfor rec in partial_records:\n    print(rec.url, rec.complete)\n</code></pre></p>"},{"location":"api/desyncclient/#pull_data","title":"pull_data()","text":"<p>Signature: Python<pre><code>def pull_data(\n    self,\n    record_id=None,\n    url=None,\n    domain=None,\n    timestamp=None,\n    bulk_search_id=None,\n    search_type=None,\n    latency_ms=None,\n    complete=None,\n    created_at=None\n) -&gt; list\n</code></pre></p> <p>Description: Retrieves full data (including text and optional HTML content) for one or more records matching the provided filters. Returns a list of <code>PageData</code> objects.</p> <p>Parameters: Any combination of filters like <code>record_id</code>, <code>url</code>, <code>domain</code>, <code>timestamp</code>, or <code>bulk_search_id</code>.</p> <p>Example: Python<pre><code>detailed_records = client.pull_data(url=\"https://example.com\")\nfor record in detailed_records:\n    print(record.html_content)\n</code></pre></p>"},{"location":"api/desyncclient/#pull_credits_balance","title":"pull_credits_balance()","text":"<p>Signature: Python<pre><code>def pull_credits_balance(self) -&gt; dict\n</code></pre></p> <p>Description: Checks the user\u2019s current credit balance and returns it as a dictionary.</p> <p>Example: Python<pre><code>balance_info = client.pull_credits_balance()\nprint(balance_info[\"credits_balance\"])\n</code></pre></p>"},{"location":"api/desyncclient/#collect_results","title":"collect_results()","text":"<p>Signature: Python<pre><code>def collect_results(\n    self,\n    bulk_search_id: str,\n    target_links: list,\n    wait_time: float = 30.0,\n    poll_interval: float = 2.0,\n    completion_fraction: float = 0.975\n) -&gt; list\n</code></pre></p> <p>Description: Polls periodically for bulk search completion until a specified fraction of pages are done or a maximum wait time elapses, then retrieves full data. Returns a list of <code>PageData</code> objects.</p> <p>Parameters:</p> <ul> <li>bulk_search_id (str): The unique identifier for the bulk search.  </li> <li>target_links (list[str]): The list of URLs in the bulk job.  </li> <li>wait_time (float): Maximum polling duration in seconds.  </li> <li>poll_interval (float): Interval between status checks.  </li> <li>completion_fraction (float): Fraction of completed results needed to stop polling.</li> </ul> <p>Example: Python<pre><code>results = client.collect_results(\n    bulk_search_id=\"bulk-id-123\",\n    target_links=[\"https://example.com\", \"https://another.com\"]\n)\nprint(len(results))\n</code></pre></p>"},{"location":"api/desyncclient/#simple_bulk_search","title":"simple_bulk_search()","text":"<p>Signature: Python<pre><code>def simple_bulk_search(\n    self,\n    target_list: list,\n    extract_html: bool = False,\n    poll_interval: float = 2.0,\n    wait_time: float = 30.0,\n    completion_fraction: float = 1\n) -&gt; list\n</code></pre></p> <p>Description: Splits a large list of URLs into chunks (up to 1000 URLs each), initiates a bulk search for each chunk, then collects and aggregates the results.</p> <p>Parameters:</p> <ul> <li>target_list (list[str]): URLs to be processed, possibly more than 1000.  </li> <li>extract_html (bool): If <code>True</code>, includes the full HTML content.  </li> <li>poll_interval (float): Polling interval in seconds.  </li> <li>wait_time (float): Maximum wait time in seconds per chunk.  </li> <li>completion_fraction (float): Fraction of completed links needed to stop polling each chunk.</li> </ul> <p>Example: Python<pre><code>all_pages = client.simple_bulk_search(\n    target_list=[\"https://site1.com\", \"https://site2.com\", ...],\n    extract_html=False\n)\nprint(len(all_pages))\n</code></pre></p>"},{"location":"api/desyncclient/#crawl","title":"crawl()","text":"<p>Signature: Python<pre><code>def crawl(\n    self,\n    start_url: str,\n    max_depth: int = 2,\n    scrape_full_html: bool = False,\n    remove_link_duplicates: bool = True,\n    poll_interval: float = 2.0,\n    wait_time_per_depth: float = 30.0,\n    completion_fraction: float = 0.975\n) -&gt; list\n</code></pre></p> <p>Description: Recursively crawls the specified <code>start_url</code> up to <code>max_depth</code> levels. Performs a stealth search on the start URL, collects same-domain links, and uses bulk searches to fetch pages at each depth.</p> <p>Parameters:</p> <ul> <li>start_url (str): Initial URL to crawl.  </li> <li>max_depth (int): Maximum crawl depth.  </li> <li>scrape_full_html (bool): If <code>True</code>, includes the full HTML.  </li> <li>remove_link_duplicates (bool): If <code>True</code>, removes duplicate links.  </li> <li>poll_interval (float): Polling interval in seconds.  </li> <li>wait_time_per_depth (float): Maximum wait time in seconds per depth.  </li> <li>completion_fraction (float): Fraction of completed links required to move to the next depth.</li> </ul> <p>Example: Python<pre><code>crawled_pages = client.crawl(\n    start_url=\"https://example.com\",\n    max_depth=3,\n    scrape_full_html=False\n)\nprint(len(crawled_pages))\n</code></pre></p>"},{"location":"api/desyncclient/#_post_and_parse","title":"_post_and_parse()","text":"<p>Signature: Python<pre><code>def _post_and_parse(self, payload)\n</code></pre></p> <p>Description: An internal helper method that sends the given payload to the API, parses the JSON response, and raises an error if the request fails.</p>"},{"location":"api/pagedata/","title":"PageData class","text":"<p>The <code>PageData</code> class packages all the information extracted from a web page during a search. It includes both details about the page itself and metadata about the search operation (such as timestamps and latency).</p>"},{"location":"api/pagedata/#pagedata-attributes","title":"PageData Attributes","text":""},{"location":"api/pagedata/#id-int","title":"<code>id</code> (int)","text":"<p>A unique identifier for the search result. This is always an integer.</p>"},{"location":"api/pagedata/#url-str","title":"<code>url</code> (str)","text":"<p>The URL targeted by the search, often referred to as the \"target URL\" or \"target page\".  For example: <code>abc.com/news</code>.</p>"},{"location":"api/pagedata/#domain-str","title":"<code>domain</code> (str)","text":"<p>The domain of the targeted URL.  For instance, if the URL is <code>abc.com/news</code>, then the domain is <code>abc.com</code>.</p>"},{"location":"api/pagedata/#timestamp-int","title":"<code>timestamp</code> (int)","text":"<p>A Unix timestamp marking when the result was received. This value is always an integer.</p>"},{"location":"api/pagedata/#bulk_search_id-str","title":"<code>bulk_search_id</code> (str)","text":"<p>A unique identifier for the bulk search batch that this search belongs to. All searches in the same batch share the same <code>bulk_search_id</code>. This may be <code>NONE</code> if the search was not part of a bulk search.</p>"},{"location":"api/pagedata/#search_type-str","title":"<code>search_type</code> (str)","text":"<p>A string indicating the type of search performed. Two options are available: - <code>stealth_search</code>: (Default) Renders JavaScript and employs methods to evade most bot detection systems, making the search appear human. - <code>test_search</code>: Does not render JavaScript and uses minimal measures to appear human. This mode is intended for prototyping and may produce different results than <code>stealth_search</code>.</p>"},{"location":"api/pagedata/#text_content-str","title":"<code>text_content</code> (str)","text":"<p>The text extracted from the page\u2019s DOM. This attribute is always a string and is recommended for use with any retrieval or regex-based data extraction.</p>"},{"location":"api/pagedata/#html_content-str","title":"<code>html_content</code> (str)","text":"<p>The full HTML (or as much as was loaded during extraction) of the target page. This attribute is optional and not returned by default to save on bandwidth and processing time. Enable it if you need the complete HTML. Every method that performs a search has a option to turn this on.</p>"},{"location":"api/pagedata/#internal_links-liststr","title":"<code>internal_links</code> (list[str])","text":"<p>A list of URLs found on the target page that point to the same domain. For example, if the target URL is <code>abc.com</code>, the list might include: Text Only<pre><code>[\"abc.com/news\", \"abc.com/faq\", ...]\n</code></pre> This is useful for navigating within the same website.</p>"},{"location":"api/pagedata/#external_links-liststr","title":"<code>external_links</code> (list[str])","text":"<p>A list of URLs on the target page that point to different domains. For example, if the target URL is <code>abc.com</code>, the list might include: Text Only<pre><code>[\"def.com\", \"def.com/news\", ...]\n</code></pre></p>"},{"location":"api/pagedata/#latency_ms-int","title":"<code>latency_ms</code> (int)","text":"<p>An integer representing the time in milliseconds (ms) between the server-side initialization of the search and the collection of results. Note that this may differ from client-side timings due to network and polling delays.</p>"},{"location":"api/pagedata/#complete-bool","title":"<code>complete</code> (bool)","text":"<p>A boolean value indicating whether the search operation is complete. This is primarily used for developer purposes.</p>"},{"location":"api/pagedata/#created_at-int","title":"<code>created_at</code> (int)","text":"<p>A Unix timestamp marking the moment the search was initiated on the client-side.</p>"},{"location":"pages/create_api_key/","title":"Create an API Key","text":"<p>   Head over to our website to generate an API key :)         Get an API Key      </p>"},{"location":"pages/installation/","title":"Installation &amp; Setup","text":""},{"location":"pages/installation/#1-installing-the-library","title":"1. Installing the Library","text":"<p>To install Desync Search via pip, run:</p> Bash<pre><code>pip install desync_search\n</code></pre>"},{"location":"pages/installation/#2-setting-up-your-api-key","title":"2. Setting Up Your API Key","text":"<p>Desync Search uses your API key to authenticate requests. The <code>DesyncClient</code> automatically checks for an environment variable named <code>DESYNC_API_KEY</code> if you don't pass the key directly. This ensures secure and convenient usage.</p>"},{"location":"pages/installation/#setting-the-environment-variable","title":"Setting the Environment Variable","text":""},{"location":"pages/installation/#on-unixlinuxmacos-bash","title":"On Unix/Linux/MacOS (bash):","text":"<p>Add the following to your terminal session or your shell profile (e.g. <code>.bashrc</code> or <code>.bash_profile</code>):</p> Bash<pre><code>export DESYNC_API_KEY=\"your_api_key_here\"\n</code></pre>"},{"location":"pages/installation/#on-windows-command-prompt","title":"On Windows (Command Prompt):","text":"<p>Run the following command:</p> Text Only<pre><code>set DESYNC_API_KEY=your_api_key_here\n</code></pre>"},{"location":"pages/installation/#on-windows-powershell","title":"On Windows (PowerShell):","text":"<p>Use this command:</p> PowerShell<pre><code>$env:DESYNC_API_KEY=\"your_api_key_here\"\n</code></pre>"},{"location":"pages/installation/#3-initializing-the-client","title":"3. Initializing the Client","text":"<p>Once your API key is set, you can initialize the client without specifying the API key:</p> Python<pre><code>from desync_search import DesyncClient\n\nclient = DesyncClient()\n</code></pre> <p>Alternatively, you can pass a different API key directly:</p> Python<pre><code>client = DesyncClient(user_api_key=\"your_api_key_here\")\n</code></pre>"},{"location":"pages/overview/","title":"Overview","text":"<p>Desync Search is a Python library made to help you extract data from the web at scale quickly, while evading bot detection. It combines low-detectability techniques, massive concurrency, and ease of integration to deliver the best performance and pricing on the market.</p> <p>Key Features:</p> <ul> <li> <p>Stealth Mode:   Operates with minimal detection, even on pages protected against bot traffic.</p> </li> <li> <p>Massive Concurrency:   Supports up to 50,000 concurrent operations, with any additional requests automatically queued.</p> </li> <li> <p>Simple Integration:   Start using Desync Search in just three lines of code:   Python<pre><code>import desync_search\n\nclient = desync_search.DesyncClient(user_api_key=\"YOUR_API_KEY\")\nresult = client.search(\"https://example.com\")\n</code></pre></p> </li> <li> <p>Pricing:</p> <p>$1 = 10,000 credits. A stealth search is 10 credits. A test search is 1 credit.</p> </li> <li> <p>Low Latency:   Experience quick response times and efficient data extraction with consistently low latency.</p> </li> </ul>"},{"location":"pages/quickstart/","title":"Quickstart with Desync Search","text":"<p>Below are ready-to-run code examples that demonstrate the core features of Desync Search. Simply copy these snippets into your IDE, update your API key if necessary (or set it in your environment), and run!</p>"},{"location":"pages/quickstart/#1-performing-a-single-search","title":"1. Performing a Single Search","text":"<p>What It Does: Search a single URL and return detailed page data\u2014including the URL, links, and content length\u2014packaged in a <code>PageData</code> object.</p> Python<pre><code>from desync_search import DesyncClient\n\nclient = DesyncClient()\ntarget_url = \"https://example.com\"\nresult = client.search(target_url)\n\nprint(\"URL:\", result.url)\nprint(\"Internal Links:\", len(result.internal_links))\nprint(\"External Links:\", len(result.external_links))\nprint(\"Text Content Length:\", len(result.text_content))\n</code></pre>"},{"location":"pages/quickstart/#2-crawling-an-entire-domain","title":"2. Crawling an Entire Domain","text":"<p>What It Does: Recursively crawls a website. The starting page is \"depth 0\". Any link on that page (that has the same domain) is considered \"depth 1\", and links on depth 1 pages become \"depth 2\", and so on. This process continues until the maximum depth is reached or no new unique pages are found.</p> Python<pre><code>from desync_search import DesyncClient\n\nclient = DesyncClient()\n\npages = client.crawl(\n    start_url=\"https://example.com\",\n    max_depth=2,\n    scrape_full_html=False,\n    remove_link_duplicates=True\n)\n\nprint(f\"Discovered {len(pages)} pages.\")\nfor page in pages:\n    print(\"URL:\", page.url, \"| Depth:\", getattr(page, \"depth\", \"N/A\"))\n</code></pre>"},{"location":"pages/quickstart/#3-initiating-a-bulk-search","title":"3. Initiating a Bulk Search","text":"<p>What It Does: Processes a list of URLs asynchronously in one operation. Up to 1000 URLs can be processed per bulk search. The method returns metadata including a unique bulk search ID that you can later use to retrieve the full results.</p> Python<pre><code>from desync_search import DesyncClient\n\nclient = DesyncClient()\nurls = [\n    \"https://example.com\",\n    \"https://another-example.com\",\n    # Add additional URLs here (up to 1000 per bulk search)\n]\n\nbulk_info = client.bulk_search(target_list=urls, extract_html=False)\nprint(\"Bulk Search ID:\", bulk_info.get(\"bulk_search_id\"))\nprint(\"Total Links Scheduled:\", bulk_info.get(\"total_links\"))\n</code></pre>"},{"location":"pages/quickstart/#4-collecting-bulk-search-results","title":"4. Collecting Bulk Search Results","text":"<p>What It Does: After initiating a bulk search, use this snippet to poll for and collect the complete results. The method waits until a specified fraction of the URLs have been processed (or a timeout is reached) and then retrieves the full page data.</p> Python<pre><code>from desync_search import DesyncClient\n\nclient = DesyncClient()\nurls = [\n    \"https://example.com\",\n    \"https://another-example.com\",\n    # Add more URLs as needed\n]\n\n# Initiate a bulk search\nbulk_info = client.bulk_search(target_list=urls, extract_html=False)\n\n# Poll and collect results once enough pages are complete\nresults = client.collect_results(\n    bulk_search_id=bulk_info[\"bulk_search_id\"],\n    target_links=urls,\n    wait_time=30.0,\n    poll_interval=2.0,\n    completion_fraction=0.975\n)\n\nprint(f\"Retrieved {len(results)} pages from the bulk search.\")\nfor result in results:\n    print(\"URL:\", result.url)\n</code></pre>"},{"location":"pages/quickstart/#5-using-simple-bulk-search","title":"5. Using Simple Bulk Search","text":"<p>What It Does: For large lists of URLs (even exceeding 1000 elements), the <code>simple_bulk_search</code> method splits the list into manageable chunks, starts a bulk search for each chunk, and then aggregates all the results. This provides a fully managed bulk search experience.</p> Python<pre><code>from desync_search import DesyncClient\n\nclient = DesyncClient()\nurls = [\n    \"https://example.com\",\n    \"https://another-example.com\",\n    # Add as many URLs as needed; this method handles splitting automatically.\n]\n\nresults = client.simple_bulk_search(\n    target_list=urls,\n    extract_html=False,\n    poll_interval=2.0,\n    wait_time=30.0,\n    completion_fraction=1\n)\n\nprint(f\"Retrieved {len(results)} pages using simple_bulk_search.\")\nfor result in results:\n    print(\"URL:\", result.url)\n</code></pre>"}]}